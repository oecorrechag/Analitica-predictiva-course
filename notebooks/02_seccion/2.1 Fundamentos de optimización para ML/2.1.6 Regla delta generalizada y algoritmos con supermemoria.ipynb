{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2073571",
   "metadata": {},
   "source": [
    "# 2.1.6 Regla delta generalizada y algoritmos con supermemoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76ea76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb90829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d816c05",
   "metadata": {},
   "source": [
    "## 2.1.6.1 Definición del problema de estimación de parámetros y función de pérdida\n",
    "\n",
    "Se desean encontra los parámetros w0 y w1 del siguiente modelo de regresión entre las variables x y y:\n",
    "\n",
    "$$ y_{i} = w_{i}x_{i} + w_{0} + e_{i} $$\n",
    "\n",
    "a partir de un conjunto de observaciones ${d_{i}, x_{i}}$, donde $d_{i}$ es el valor real (observado), $y_{i}$ es su aproximación usando el modelo de regresión descrito, y $e_{i}$ es el error aleatorio.\n",
    "\n",
    "- Error instantáneo:\n",
    "\n",
    "$$ e_{i} = d_{i} - y_{i} $$\n",
    "\n",
    "- Sumatoria del error cuadrático instantáneo:\n",
    "\n",
    "$$ SSE(w_0, w_1) = \\sum_{i}^{} (d_i - y_i)^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd10b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Dataset\n",
    "#\n",
    "x_sample = [\n",
    "    0.1087,0.2698,0.3765,0.2146,0.9155,0.0246,\n",
    "    0.0221,0.8632,0.6460,0.2092,0.8567,0.1591,\n",
    "    0.9647,0.6231,0.7460,0.3654,0.3065,0.6886,\n",
    "    0.4966,0.2008,0.2618,0.7607,0.1563,0.4424,\n",
    "    0.7731,\n",
    "]\n",
    "\n",
    "y_sample = [\n",
    "    0.9519,1.1237,1.2360,1.0526,2.0743,0.7906,\n",
    "    0.7603,2.0533,1.6887,1.0563,2.0991,0.8953,\n",
    "    2.1917,1.6266,1.8508,1.2828,1.2283,1.8722,\n",
    "    1.4657,1.0418,1.1097,1.7826,0.9711,1.4267,\n",
    "    1.8248,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165d9e9",
   "metadata": {},
   "source": [
    "## 2.1.6.2 Función de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec85e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Sumatoria del error cuadrático o pérdida cuadrática\n",
    "#\n",
    "def sse(w0, w1):\n",
    "    y_forecasts = [w0 + w1 * x_value for x_value in x_sample]\n",
    "    errors = [y_true - y_forecast for y_true, y_forecast in zip(y_sample, y_forecasts)]\n",
    "    squared_errors = [error ** 2 for error in errors]\n",
    "    return sum(squared_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9327e912",
   "metadata": {},
   "source": [
    "## 2.1.6.3 Método del gradiente\n",
    "\n",
    "$$ w_k = w_{k-1} - \\mu \\frac{\\delta}{\\delta_w} SEE(\\delta_{k-1}) $$\n",
    "\n",
    "con \n",
    "\n",
    "$$ \\frac{\\delta}{\\delta_w} SEE(W) = [ \\begin{matrix} -2 \\sum_{i}{} e_i \\\\ -2 \\sum_{i}{} e_i x_i \\end{matrix} ] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d7325",
   "metadata": {},
   "source": [
    "## 2.1.6.4 Derivadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1310a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Esta es el mismo computo que el empleado en el\n",
    "# método batch.\n",
    "#\n",
    "def gradient(w0, w1):\n",
    "\n",
    "    y_forecasts = [w0 + w1 * x_value for x_value in x_sample]\n",
    "\n",
    "    errors = [y_true - y_forecast for y_true, y_forecast in zip(y_sample, y_forecasts)]\n",
    "\n",
    "    gradient_w0 = -2 * sum(errors)\n",
    "    gradient_w1 = -2 * sum([error * x_value for error, x_value in zip(errors, x_sample)])\n",
    "\n",
    "    return gradient_w0, gradient_w1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1681777d",
   "metadata": {},
   "source": [
    "## 2.1.6.5 Ecuación de mejora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72195078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve(w0, w1, mu):\n",
    "\n",
    "    gradient_w0, gradient_w1 = gradient(w0, w1)\n",
    "\n",
    "    w0 = w0 - mu * gradient_w0\n",
    "    w1 = w1 - mu * gradient_w1\n",
    "\n",
    "    return w0, w1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569ea347",
   "metadata": {},
   "source": [
    "## 2.1.6.6 Proceso iterativo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8300e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contour():\n",
    "\n",
    "    W0 = np.linspace(-0.4, 1.5, 100)\n",
    "    W1 = np.linspace(1.2, 3.0, 100)\n",
    "    W0, W1 = np.meshgrid(W0, W1)\n",
    "    F = sse(W0, W1)\n",
    "\n",
    "    levels = [0, 0.2, 0.5, 0.75, 1, 2, 3, 5, 10, 20, 30, 40, 50, 60]\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.set_aspect(\"equal\", \"box\")\n",
    "    ax.contourf(W0, W1, F, cmap=cm.Greys, levels=levels, alpha=0.6)\n",
    "    ax.contour(W0, W1, F, colors=\"gray\", levels=levels)\n",
    "    plt.plot(\n",
    "        [0.731],\n",
    "        [1.498],\n",
    "        \"o\",\n",
    "        color=\"black\",\n",
    "        fillstyle=\"none\",\n",
    "        markersize=11,\n",
    "        markeredgewidth=2,\n",
    "    )\n",
    "    plt.plot([0.731], [1.498], \".\", color=\"black\")\n",
    "    ax.set_xlabel(\"w0\")\n",
    "    ax.set_ylabel(\"w1\")\n",
    "    ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8fe647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto de inicio\n",
    "w0 = 0.5\n",
    "w1 = 2.9\n",
    "\n",
    "history_gd = {\n",
    "    \"w0\": [w0],\n",
    "    \"w1\": [w1],\n",
    "    \"sse\": [sse(w0, w1)],\n",
    "}\n",
    "\n",
    "mu = 0.03\n",
    "\n",
    "for epoch in range(20):\n",
    "\n",
    "    w0, w1 = improve(w0, w1, mu)\n",
    "\n",
    "    history_gd[\"w0\"].append(w0)\n",
    "    history_gd[\"w1\"].append(w1)\n",
    "    history_gd[\"sse\"].append(sse(w0, w1))\n",
    "\n",
    "print(\" w0 = {:6.4f}\\n w1 = {:6.4f}\\nSSE = {:6.4f}\".format(w0, w1, sse(w0, w1)))\n",
    "\n",
    "plot_contour()\n",
    "plt.plot(history_gd[\"w0\"], history_gd[\"w1\"], marker=\"o\", color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5640ff8",
   "metadata": {},
   "source": [
    "## 2.1.6.7 Momentum\n",
    "\n",
    "Para evitar el zigzag, se desea usar la dirección del gradiente actual ponderada por la dirección previa. Esta metodología se conoce como regla delta (en la jerga de redes neuronales artificiales) o gradiente con memoria (en investigación de operaciones).\n",
    "\n",
    "Específicamente, el proceso de optimización se basa en la siguiente ecuación recursiva (ya estudiada):\n",
    "\n",
    "$$ w_k = w_{k-1} - \\mu \\frac{\\delta}{\\delta_w} SEE(w_{k-1}) $$  \n",
    "\n",
    "la cual que puede ser reescrita como:\n",
    "\n",
    "$$ w_k = w_{k-1} - \\Lambda w_{k-1} $$\n",
    "\n",
    "con:\n",
    "\n",
    "$$ \\Lambda w_{k-1} = -  \\mu \\frac{\\delta}{\\delta_w} SEE(w_{k-1})  $$\n",
    "\n",
    "La dirección tomada en la iteración anterior será Δwk−2. Ahora, solo es necesario agregar este término a la ecuación anterior, ponderado por un parámetro β conocido comúnmente como momentum:\n",
    "\n",
    "$$ \\Lambda w_{k-1} = -  \\mu \\frac{\\delta}{\\delta_w} SEE(w_{k-1}) + \\beta \\Lambda w_{k-2}  $$\n",
    "\n",
    "## 2.1.6.8 Proceso iterativo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c7695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve(w0, w1, mu, beta):\n",
    "\n",
    "    # Corrección en la iteración anterior.\n",
    "    global previous_delta_w0\n",
    "    global previous_delta_w1\n",
    "\n",
    "    # Computó del gradiente para los parámetros actuales\n",
    "    gradient_w0, gradient_w1 = gradient(w0, w1)\n",
    "\n",
    "    # Corrección de los parámetros\n",
    "    delta_w0 = -mu * gradient_w0 + beta * previous_delta_w0\n",
    "    delta_w1 = -mu * gradient_w1 + beta * previous_delta_w1\n",
    "\n",
    "    # Almacenamiento de la corrección para la siguiente iteración\n",
    "    previous_delta_w0 = delta_w0\n",
    "    previous_delta_w1 = delta_w1\n",
    "\n",
    "    w0 = w0 + delta_w0\n",
    "    w1 = w1 + delta_w1\n",
    "\n",
    "    return w0, w1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910045b3",
   "metadata": {},
   "source": [
    "El proceso de iterativo de optimización es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45a3f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punto de inicio\n",
    "w0 = 0.5\n",
    "w1 = 2.9\n",
    "\n",
    "history_dr = {\n",
    "    \"w0\": [w0],\n",
    "    \"w1\": [w1],\n",
    "    \"sse\": [sse(w0, w1)],\n",
    "}\n",
    "\n",
    "previous_delta_w0 = 0\n",
    "previous_delta_w1 = 0\n",
    "\n",
    "for epoch in range(20):\n",
    "\n",
    "    w0, w1 = improve(w0, w1, mu=0.03, beta=0.7)\n",
    "\n",
    "    history_dr[\"w0\"].append(w0)\n",
    "    history_dr[\"w1\"].append(w1)\n",
    "    history_dr[\"sse\"].append(sse(w0, w1))\n",
    "\n",
    "print(\" w0 = {:6.4f}\\n w1 = {:6.4f}\\nSSE = {:6.4f}\".format(w0, w1, sse(w0, w1)))\n",
    "\n",
    "plot_contour()\n",
    "plt.plot(history_gd[\"w0\"], history_gd[\"w1\"], marker=\"o\", color=\"black\")\n",
    "plt.plot(history_dr[\"w0\"], history_dr[\"w1\"], marker=\"o\", color=\"tab:blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46ddcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_gd[\"sse\"], \"o-k\", label=\"gradient descendent\")\n",
    "plt.plot(history_dr[\"sse\"], \"o-\", color=\"tab:blue\", label=\"delta rule\")\n",
    "plt.xlabel(\"Iteraciones\")\n",
    "plt.ylabel(\"SSE(w0, w1)\")\n",
    "plt.gca().spines[\"left\"].set_color(\"gray\")\n",
    "plt.gca().spines[\"bottom\"].set_color(\"gray\")\n",
    "plt.gca().spines[\"top\"].set_visible(False)\n",
    "plt.gca().spines[\"right\"].set_visible(False)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve(w0, w1, mu):\n",
    "\n",
    "    gradient_w0, gradient_w1 = gradient(w0, w1)\n",
    "\n",
    "    w0 = w0 - mu * gradient_w0\n",
    "    w1 = w1 - mu * gradient_w1\n",
    "\n",
    "    return w0, w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a51fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve2(w0, w1, mu, beta):\n",
    "\n",
    "    # Corrección en la iteración anterior.\n",
    "    global previous_delta_w0\n",
    "    global previous_delta_w1\n",
    "\n",
    "    # Computó del gradiente para los parámetros actuales\n",
    "    gradient_w0, gradient_w1 = gradient(w0, w1)\n",
    "\n",
    "    # Corrección de los parámetros\n",
    "    delta_w0 = -mu * gradient_w0 + beta * previous_delta_w0\n",
    "    delta_w1 = -mu * gradient_w1 + beta * previous_delta_w1\n",
    "\n",
    "    # Almacenamiento de la corrección para la siguiente iteración\n",
    "    previous_delta_w0 = delta_w0\n",
    "    previous_delta_w1 = delta_w1\n",
    "\n",
    "    w0 = w0 + delta_w0\n",
    "    w1 = w1 + delta_w1\n",
    "\n",
    "    return w0, w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4809ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epocas(w0, w1, epocas, learning_rate, factor_beta):\n",
    "    \n",
    "    w0g, w1g = w0, w1\n",
    "    \n",
    "    history_gd = {\"w0\": [w0], \"w1\": [w1], \"sse\": [sse(w0, w1)]}\n",
    "    history_dr = {\"w0\": [w0],\"w1\": [w1],\"sse\": [sse(w0, w1)]}\n",
    "    \n",
    "    previous_delta_w0 = 0\n",
    "    previous_delta_w1 = 0\n",
    "    \n",
    "    for epoch in range(epocas):\n",
    "        \n",
    "        w0g, w1g = improve(w0g, w1g, learning_rate)\n",
    "\n",
    "        history_gd[\"w0\"].append(w0g)\n",
    "        history_gd[\"w1\"].append(w1g)\n",
    "        history_gd[\"sse\"].append(sse(w0g, w1g))\n",
    "        \n",
    "        \n",
    "        w0, w1 = improve2(w0, w1, learning_rate, factor_beta)\n",
    "            \n",
    "        history_dr[\"w0\"].append(w0)\n",
    "        history_dr[\"w1\"].append(w1)\n",
    "        history_dr[\"sse\"].append(sse(w0, w1))\n",
    "    \n",
    "    #\n",
    "    # Ultimo resultado obtenido\n",
    "    #\n",
    "    # print(\" w0 = {:6.4f}\\n w1 = {:6.4f}\\nSSE = {:6.4f}\".format(w0, w1, sse(w0, w1)))\n",
    "    \n",
    "    return history_gd, history_dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea3734",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_gd, history_dr = epocas(0.5, 2.9, 15, 0.02, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65547f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_contour()\n",
    "plt.plot(history_gd[\"w0\"], history_gd[\"w1\"], marker=\"o\", color=\"black\")\n",
    "plt.plot(history_dr[\"w0\"], history_dr[\"w1\"], marker=\"o\", color=\"tab:blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f47970",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history_gd[\"sse\"], \"o-k\", label=\"gradient descendent\")\n",
    "plt.plot(history_dr[\"sse\"], \"o-\", color=\"tab:blue\", label=\"delta rule\")\n",
    "plt.xlabel(\"Iteraciones\")\n",
    "plt.ylabel(\"SSE(w0, w1)\")\n",
    "plt.gca().spines[\"left\"].set_color(\"gray\")\n",
    "plt.gca().spines[\"bottom\"].set_color(\"gray\")\n",
    "plt.gca().spines[\"top\"].set_visible(False)\n",
    "plt.gca().spines[\"right\"].set_visible(False)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66445448",
   "metadata": {},
   "source": [
    "## 2.1.6.9 Algoritmos con supermemoria\n",
    "\n",
    "Son algoritmos de estructura similar a la regla delta, pero consideran P valores de Δw hacia atrás.\n",
    "\n",
    "$$ \\Lambda w_{k-1} = -  \\mu \\frac{\\delta}{\\delta_w} SEE(w_{k-1}) + \\sum_{p=0}^{p} \\beta_p \\Lambda w_{k-2-p}  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3345c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ok_')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
