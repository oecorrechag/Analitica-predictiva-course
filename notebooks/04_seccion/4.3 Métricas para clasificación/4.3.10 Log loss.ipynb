{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bfc8fc6",
   "metadata": {},
   "source": [
    "# 4.3.10 Log loss (logistic regression loss, cross-entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1794033b",
   "metadata": {},
   "source": [
    "Se usa comunmente en regresión logística y redes neuronales artificiales.\n",
    "\n",
    "Para el caso binario con $y \\ \\epsilon \\ \\{0,1\\}$ y $p=Pr(y=1)$ se define como el negativo de la función log-likelihood:\n",
    "\n",
    "$$ L_{log}(y,p) = - log Pr(y | p) = - y \\ log(p) - (1-p) log (1-p) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a6446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entropy():\n",
    "\n",
    "    x = np.linspace(0.0001, 0.9999, 100)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, -np.log(x))\n",
    "    plt.xlabel(\"Probabilidad pronosticada (p) con y=1\")\n",
    "    plt.ylabel(\"$-\\log(p)$\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, -np.log(1 - x))\n",
    "    plt.xlabel(\"Probabilidad pronosticada (p) con y=0\")\n",
    "    plt.ylabel(\"$-\\log(p)$\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f7dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0, 0, 1, 1]\n",
    "y_pred = [[0.9, 0.1], [0.8, 0.2], [0.3, 0.7], [0.01, 0.99]]\n",
    "\n",
    "#\n",
    "# - 0 * log(0.1)  - (1 - 0) log(0.9)  = -log(0.9)\n",
    "# - 0 * log(0.2)  - (1 - 0) log(0.8)  = -log(0.8)\n",
    "# - 1 * log(0.7)  - (1 - 1) log(0.3)  = -log(0.7)\n",
    "# - 1 * log(0.99) - (1 - 1) log(0.01) = -log(0.99)\n",
    "#\n",
    "# (- log(0.9) - log(0.8) - log(0.7) - log(0.99)) / 4 = 0.1738073\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Ground truth (correct) target values.\n",
    "    y_true=y_true,\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Predicted probabilities, as returned by a classifier’s predict_proba\n",
    "    # method\n",
    "    y_pred=y_pred,\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Log loss is undefined for p=0 or p=1, so probabilities are clipped to\n",
    "    # max(eps, min(1 - eps, p))\n",
    "    eps=1e-15,\n",
    "    # -------------------------------------------------------------------------\n",
    "    # If true, return the mean loss per sample. Otherwise, return the sum of\n",
    "    # the per-sample losses.\n",
    "    normalize=True,\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sample weights.\n",
    "    sample_weight=None,\n",
    "    # -------------------------------------------------------------------------\n",
    "    # If not provided, labels will be inferred from y_true.\n",
    "    labels=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e51b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss(\n",
    "    y_true=[\"ham\", \"ham\", \"spam\", \"spam\"],\n",
    "    y_pred=y_pred,\n",
    "    labels=[\"ham\", \"spam\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de9c16",
   "metadata": {},
   "source": [
    "Para multiples clases se generaliza de la siguiente forma:\n",
    "\n",
    "- $Y$ es una matriz de indicadores binarios con $y_{i,k}=1$ si la $i-ésima$ muestra tiene la etiqueta $k$ perteneciente a un conjunto de $K$ etiquedas.\n",
    "- $P$ es la matriz de estimados de probabilidad con $p_{i,k}=Pr(y_{i,k}=1)$\n",
    "\n",
    "$$ L_{log}(Y,P) = - log Pr(Y | P) = - \\frac{1}{N} \\sum_{i=0}^{N-1} \\sum_{k=0}^{K-1} y_{i,k} log p_{i,k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e0489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ok_')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
