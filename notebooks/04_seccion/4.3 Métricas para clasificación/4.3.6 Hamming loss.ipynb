{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bfc8fc6",
   "metadata": {},
   "source": [
    "# 4.3.6 Hamming loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d90735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9885dbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3fd997",
   "metadata": {},
   "source": [
    "- Esta función computa el promedio de la función de pérdida de Hamming o distancia de Hamming entre dos conjuntos de muestras.\n",
    "- La distancia de Hamming entre dos strings de igual longitud es el número de posiciones en que los símbolos son diferentes. Tambien se define como la cantidad mínima de sustituciones requeridas para transformar un string en otro.\n",
    "- La función de pérdida se define como:\n",
    "\n",
    "    $$ L_{Hamming}(y, \\hat{y}) = \\frac{1}{n_{samples} - n_{labels}}  \\sum_{i=0}^{n_{samples}-1}   \\sum_{j=0}^{n_{labels}-1}   1(\\hat{y_{i,j}} \\not= y_{i,j})  $$\n",
    "\n",
    "    donde:\n",
    "\n",
    "    - $\\hat{y_{i,j}}$ es el valor pronosticado para la $j-ésima$ etiqueta de la muestra $i$\n",
    "    - $y_{i,j}$ es el valor verdadero.\n",
    "    - $n_{samples}$ es el número de muestras.\n",
    "    - $n_{labels}$ es el número de etiquetas.\n",
    "\n",
    "- La ecuación anterior no puede ser usada para clasificación con múltiples clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd52448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = [1, 2, 3, 4]\n",
    "y_true = [2, 2, 3, 4]\n",
    "\n",
    "hamming_loss(\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Ground truth (correct) labels.\n",
    "    y_true=y_true,\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Predicted labels, as returned by a classifier.\n",
    "    y_pred=y_pred,\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Sample weights.\n",
    "    sample_weight=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67491f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# y_true:\n",
    "#   [[0, 1],\n",
    "#    [1, 1]]\n",
    "#\n",
    "# y_pred:\n",
    "#   [[0, 0],\n",
    "#    [0, 0]]\n",
    "#\n",
    "\n",
    "hamming_loss(\n",
    "    y_true=np.array([[0, 1], [1, 1]]),\n",
    "    y_pred=np.zeros((2, 2)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e0489e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok_\n"
     ]
    }
   ],
   "source": [
    "print('ok_')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
