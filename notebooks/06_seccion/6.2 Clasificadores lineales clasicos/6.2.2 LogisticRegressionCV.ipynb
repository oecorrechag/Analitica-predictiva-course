{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bfc8fc6",
   "metadata": {},
   "source": [
    "# 6.2.2 LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b57b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb86f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementa la regresión logística con validación cruzada para el parámetro C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegressionCV = LogisticRegressionCV(\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Each of the values in Cs describes the inverse of regularization strength.\n",
    "    # If Cs is as an int, then a grid of Cs values are chosen in a logarithmic\n",
    "    # scale between 1e-4 and 1e4. Like in support vector machines, smaller\n",
    "    # values specify stronger regularization.\n",
    "    Cs=[1e-3, 1e-2, 1e-1, 1],\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Specifies if a constant (a.k.a. bias or intercept) should be added to the\n",
    "    # decision function.\n",
    "    fit_intercept=True,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # The default cross-validation generator used is Stratified K-Folds. If an\n",
    "    # integer is provided, then it is the number of folds used.\n",
    "    cv=None,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Specify the norm of the penalty:\n",
    "    # * 'l2': add a L2 penalty term and it is the default choice.\n",
    "    # * 'l1': add a L1 penalty term.\n",
    "    # * 'elasticnet': both L1 and L2 penalty terms are added.\n",
    "    penalty=\"l2\",\n",
    "    # --------------------------------------------------------------------------\n",
    "    # A string (see model evaluation documentation) or a scorer callable\n",
    "    # object / function with signature scorer(estimator, X, y).\n",
    "    scoring=None,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Algorithm to use in the optimization problem. Default is ‘lbfgs’. To\n",
    "    # choose a solver, you might want to consider the following aspects:\n",
    "    # * For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and\n",
    "    #   ‘saga’ are faster for large ones.\n",
    "    # * For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’\n",
    "    #   handle multinomial loss.\n",
    "    # * ‘liblinear’ and is limited to one-versus-rest schemes.\n",
    "    # * ‘newton-cholesky’ is a good choice for n_samples >> n_features,\n",
    "    #   especially with one-hot encoded categorical features with rare\n",
    "    #   categories. Note that it is limited to binary classification and the\n",
    "    #   one-versus-rest reduction for multiclass classification. Be aware that\n",
    "    #   the memory usage of this solver has a quadratic dependency on n_features\n",
    "    #   because it explicitly computes the Hessian matrix.\n",
    "    solver=\"lbfgs\",\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Tolerance for stopping criteria.\n",
    "    tol=0.0001,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Maximum number of iterations taken for the solvers to converge.\n",
    "    max_iter=1000,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Weights associated with classes in the form {class_label: weight}. If not\n",
    "    # given, all classes are supposed to have weight one.\n",
    "    # * 'balanced' uses the values of y to automatically adjust weights\n",
    "    # inversely proportional to class frequencies in the input data as\n",
    "    # n_samples / (n_classes * np.bincount(y)).\n",
    "    class_weight=None,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # If set to True, the scores are averaged across all folds, and the coefs\n",
    "    # and the C that corresponds to the best score is taken, and a final refit\n",
    "    # is done using these parameters.\n",
    "    #\n",
    "    # Otherwise the coefs, intercepts and C that correspond to the best scores\n",
    "    # across folds are averaged.\n",
    "    refit=True,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Used when solver == ‘sag’, ‘saga’ or ‘liblinear’ to shuffle the data.\n",
    "    random_state=None,\n",
    "    # --------------------------------------------------------------------------\n",
    "    # {‘ovr’, ‘multinomial’, ‘auto’}\n",
    "    # If the option chosen is ‘ovr’, then a binary problem is fit for each\n",
    "    # label. For ‘multinomial’ the loss minimised is the multinomial loss fit\n",
    "    # across the entire probability distribution, even when the data is binary.\n",
    "    multi_class=\"auto\",\n",
    "    # --------------------------------------------------------------------------\n",
    "    # The Elastic-Net mixing parameter, with 0 <= l1_ratio <= 1. Only used if\n",
    "    # penalty='elasticnet'. Setting l1_ratio=0 is equivalent to using\n",
    "    # penalty='l2', while setting l1_ratio=1 is equivalent to using penalty='l1'.\n",
    "    # For 0 < l1_ratio <1, the penalty is a combination of L1 and L2.\n",
    "    l1_ratios=None,\n",
    ")\n",
    "\n",
    "logisticRegressionCV.fit(X, y)\n",
    "\n",
    "logisticRegressionCV.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28.31743674])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegressionCV.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00772273,  0.18284397, -0.27691529,  0.02276769, -0.1744071 ,\n",
       "        -0.22270091, -0.52790486, -0.28891103, -0.26172059, -0.03074184,\n",
       "        -0.07981472,  1.27171883,  0.10702267, -0.10832142, -0.02439403,\n",
       "         0.06190287, -0.03811407, -0.03726069, -0.03710315,  0.0130745 ,\n",
       "         0.11818131, -0.43939112, -0.10458489, -0.01349904, -0.3486427 ,\n",
       "        -0.70110083, -1.41004791, -0.59123028, -0.72390754, -0.09627315]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegressionCV.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 3.24733804e-14],\n",
       "       [9.99995936e-01, 4.06382637e-06],\n",
       "       [9.99999453e-01, 5.47165776e-07],\n",
       "       [6.88315904e-01, 3.11684096e-01],\n",
       "       [9.99754795e-01, 2.45204605e-04],\n",
       "       [7.53323104e-01, 2.46676896e-01],\n",
       "       [9.99994225e-01, 5.77511152e-06],\n",
       "       [9.89198526e-01, 1.08014740e-02],\n",
       "       [9.41333360e-01, 5.86666397e-02],\n",
       "       [9.98536930e-01, 1.46307041e-03]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegressionCV.predict_proba(X)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9578207381370826"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logisticRegressionCV.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f2f003d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok_\n"
     ]
    }
   ],
   "source": [
    "print('ok_')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
